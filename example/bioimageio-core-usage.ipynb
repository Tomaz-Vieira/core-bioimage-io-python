{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0daf2216",
   "metadata": {},
   "source": [
    "# bioimageio.core usage examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72ba149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "\n",
    "import bioimageio.core\n",
    "import imageio\n",
    "# we use napari for visualising images, you can install it via `pip install napari` or`conda install napari`\n",
    "import napari\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "from bioimageio.core.prediction_pipeline import create_prediction_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74613461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for showing multiple images in napari\n",
    "def show_images(*images, names=None):\n",
    "    v = napari.Viewer()\n",
    "    for i, im  in enumerate(images):\n",
    "        name = None if names is None else names[i]\n",
    "        if isinstance(im, str):\n",
    "            im = imageio.imread(im)\n",
    "        v.add_image(im, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2deb81f",
   "metadata": {},
   "source": [
    "## Loading a model\n",
    "\n",
    "We will use a model that predicts foreground and boundaries in images of nuclei from the [kaggle nucles segmentation challenge](https://www.kaggle.com/c/data-science-bowl-2018).\n",
    "Find the model on bioimage.io here: https://bioimage.io/#/?id=10.5072%2Fzenodo.881940\n",
    "\n",
    "First, we will use `bioimageio.core.load_resource_description` to load the model and inspec the obtained model resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de51dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model can be loaded using different representations:\n",
    "\n",
    "# the doi of the zenodo entry corresponding to the model\n",
    "rdf_doi = \"10.5281/zenodo.6287342\"\n",
    "\n",
    "# the url of the yaml file containing the model resource description\n",
    "rdf_url = \"https://zenodo.org/record/6287342/files/rdf.yaml\"\n",
    "\n",
    "# filepath to the downloaded model (either zipped package or yaml)\n",
    "# to download it from the website:\n",
    "# - go to https://bioimage.io/#/?id=10.5281%2Fzenodo.5764892%2F5764893\n",
    "# - click the download icon\n",
    "# - select \"ilastik\" weight format\n",
    "rdf_path = \"/home/pape/Downloads/nuclei-segmentation-boundarymodel_pytorch_state_dict.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce884f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from link to rdf.yaml\n",
    "model_resource = bioimageio.core.load_resource_description(rdf_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1230b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from doi\n",
    "model_resource = bioimageio.core.load_resource_description(rdf_doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8329315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from path to the zipped model files\n",
    "model_resource = bioimageio.core.load_resource_description(rdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23b9f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the \"model_resource\" instance returned by load_resource_description\n",
    "# contains the information stored in the resource description (see https://github.com/bioimage-io/spec-bioimage-io/blob/gh-pages/model_spec_latest.md)\n",
    "\n",
    "# we can e.g. check what weight formats are available in the model (pytorch_state_dict for the model used here)\n",
    "print(\"Available weight formats for this model:\", model_resource.weights.keys())\n",
    "# or how the weight files are stored\n",
    "print(\"Pytorch state dict weights are stored at:\", model_resource.weights[\"pytorch_state_dict\"].source)\n",
    "print()\n",
    "# or what inputs the model expects\n",
    "print(\"The model requires as inputs:\")\n",
    "for inp in model_resource.inputs:\n",
    "    print(\"Input with axes:\", inp.axes, \"and shape\", inp.shape)\n",
    "print()\n",
    "# and what the model outputs are\n",
    "print(\"The model returns the following outputs:\")\n",
    "for out in model_resource.outputs:\n",
    "    print(\"Output with axes:\", out.axes, \"and shape\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f21529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function 'test_model' from 'bioimageio.core.resource_tests' can be used to fully test the model,\n",
    "# including running prediction for the test input(s) and checking that they agree with the test output(s)\n",
    "# before using a model, it is recommended to check that it properly works with this function\n",
    "# 'test_model' returns a dict, if there are any errros they will be in the key \"error\"\n",
    "# if the model passes it will be None\n",
    "from bioimageio.core.resource_tests import test_model\n",
    "test_result = test_model(model_resource)\n",
    "if test_result[\"error\"]:\n",
    "    print(\"The model test failed with:\", test_result[\"error\"])\n",
    "    print(\"with the traceback:\", test_result[\"traceback\"])\n",
    "else:\n",
    "    print(\"The model passed all tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c75014",
   "metadata": {},
   "source": [
    "## Prediction with the model\n",
    "\n",
    "`bioimageio.core` implements functionality to run predictions with a model in bioimage.io format.\n",
    "This includes functions to run prediction with numpy arrays (more precisely xarray DataArrays) and convenience functions to run predictions for inputs stored on disc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59b1b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the example image for this model, which is stored in numpy file format\n",
    "input_image = np.load(model_resource.test_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808e2ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to run prediction on a numpy input\n",
    "# \"devices\" can be used to run prediction on a gpu instead of the cpu\n",
    "# \"weight_format\" to specify which weight format to use in case the model contains different weight formats\n",
    "def predict_numpy(model, input_, devices=None, weight_format=None):\n",
    "    # the prediction pipeline combines preprocessing, prediction and postprocessing.\n",
    "    # it should always be used for prediction with a bioimageio model\n",
    "    pred_pipeline = create_prediction_pipeline(\n",
    "        bioimageio_model=model, devices=devices, weight_format=weight_format\n",
    "    )\n",
    "\n",
    "    # the prediction pipeline expects inputs as xarray.DataArrays.\n",
    "    # these are similar to numpy arrays, but allow for named dimensions (the dims keyword argument)\n",
    "    # in bioimage.io the dims have to agree with the input axes required by the model\n",
    "    axes = tuple(model.inputs[0].axes)\n",
    "    input_tensor = xr.DataArray(input_, dims=axes)\n",
    "    \n",
    "    # the prediction pipeline call expects the same number of inputs as the number of inputs required by the model\n",
    "    # in the case here, the model just expects a single input. in the case of multiple inputs use\n",
    "    # prediction = pred_pipeline(input1, input2, ...)\n",
    "    # or, if you have the inputs in a list or tuple\n",
    "    # prediction = pred_pipeline(*inputs)\n",
    "    # the call returns a list of output tensors, corresponding to the output tensors of the model\n",
    "    # (in this case, we just have a single output)\n",
    "    prediction = pred_pipeline(input_tensor)[0]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f235e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run prediction for the test input and show the result\n",
    "prediction = predict_numpy(model_resource, input_image)\n",
    "show_images(input_image, prediction, names=[\"image\", \"prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f6c459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the utility function `predict_image` can be used to run prediction with an image stored on disc\n",
    "from bioimageio.core.prediction import predict_image\n",
    "\n",
    "# the filepath where the output should be stored, supports most common image formats as well as npy fileformat\n",
    "outputs = [\"prediction.tif\"]\n",
    "predict_image(\n",
    "    model_resource, model_resource.test_inputs, outputs\n",
    ")\n",
    "\n",
    "# the output tensor contains 2 channels, which is not supported by normal tif.\n",
    "# thus, these 2 channels are stored as 2 separate images\n",
    "fg_pred = imageio.imread(\"prediction-c0.tif\")\n",
    "bd_pred = imageio.imread(\"prediction-c1.tif\")\n",
    "show_images(input_image, fg_pred, bd_pred,\n",
    "            names=[\"image\", \"foreground-prediction\", \"boundary-prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879618df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the utility function `predict_images` can be use to run prediction for a batch of images stored on disc\n",
    "# note: this only works for models which have a single input and output!\n",
    "from bioimageio.core.prediction import predict_images\n",
    "\n",
    "# here, we use a subset of the dsb challenge data for prediction from the stardist (https://github.com/stardist/stardist)\n",
    "# you can obtain it from: https://github.com/stardist/stardist/releases/download/0.1.0/dsb2018.zip\n",
    "\n",
    "# select all images in the \"test\" subfolder\n",
    "from glob import glob\n",
    "folder = \"/home/pape/Downloads/dsb2018(1)/dsb2018/test\"\n",
    "inputs = glob(os.path.join(folder, \"images\", \"*.tif\"))\n",
    "\n",
    "# create an output folder and specify the output path for each image\n",
    "output_folder = os.path.join(folder, \"predictions\")\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "outputs = [os.path.join(output_folder, os.path.split(inp)[1]) for inp in inputs]\n",
    "\n",
    "print(len(inputs), \"images for prediction were found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7cf46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model at hand can only predict images which have a xy-size that is\n",
    "# a multiple of 16. To run with arbitrary size images, we pass the `padding`\n",
    "# argument to `predict_images` and specify that the input is padded to the next bigger\n",
    "# size that is divisible by 16 (mode: dynamic)\n",
    "# as an alternative `\"mode\": \"fixed\"` will pad to a fixed shape, e.g.\n",
    "# `{\"x\": 512, \"y\": 512, \"mode\": \"fixed\"}` will always pad to a size of 512x512\n",
    "# the padding is cropped again after the prediction\n",
    "padding = {\"x\": 16, \"y\": 16, \"mode\": \"dynamic\"}\n",
    "predict_images(\n",
    "    model_resource, inputs, outputs, padding=padding, verbose=True\n",
    ")\n",
    "\n",
    "# check the first input/output\n",
    "show_images(inputs[0], outputs[0].replace(\".tif\", \"-c0.tif\"), outputs[0].replace(\".tif\", \"-c1.tif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abe0590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of padding, we can also use tiling.\n",
    "# here, we specify a tile size of 224 and a halo (= extension of tile on both sides)\n",
    "# size of 16, which results in an effective tile shale of 256 = 224 + 2*16\n",
    "tiling = {\n",
    "    \"tile\": {\"x\": 224, \"y\": 224},\n",
    "    \"halo\": {\"x\": 16, \"y\": 16},\n",
    "}\n",
    "predict_images(\n",
    "    model_resource, inputs, outputs, tiling=tiling, verbose=True\n",
    ")\n",
    "\n",
    "# check the first input/output\n",
    "show_images(inputs[0], outputs[0].replace(\".tif\", \"-c0.tif\"), outputs[0].replace(\".tif\", \"-c1.tif\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8fd710",
   "metadata": {},
   "source": [
    "## Create a biomiage.io model package\n",
    "\n",
    "`bioimageio.core` also implements functionality to create a model package compatible with the [bioimageio model spec](https://github.com/bioimage-io/spec-bioimage-io/blob/gh-pages/model_spec_latest.md) ready to be shared via\n",
    "the [bioimage.io model zoo](https://bioimage.io/#/).\n",
    "Here, we will use this functionality to create two models, one that adds thresholding as post-processing to the outputs and another one that also adds weights in torchscript format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a210a1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the python file defining the architecture.\n",
    "# this is only required for models with pytorch_state_dict weights\n",
    "def get_architecture_source(rdf):\n",
    "    # here, we need the raw resource, which contains the information from the resource description\n",
    "    # before evaluation, e.g. the file and name of the python file with the model architecture\n",
    "    raw_resource = bioimageio.core.load_raw_resource_description(rdf)\n",
    "    # the python file defining the architecture for the pytorch weihgts\n",
    "    model_source = raw_resource.weights[\"pytorch_state_dict\"].architecture\n",
    "    # download the source file if necessary\n",
    "    source_file = bioimageio.core.resource_io.utils.resolve_source(\n",
    "        model_source.source_file\n",
    "    )\n",
    "    # if the source file path does not exist, try combining it with the root path of the model\n",
    "    if not os.path.exists(source_file):\n",
    "        source_file = os.path.join(raw_resource.root_path, os.path.split(source_file)[1])\n",
    "    assert os.path.exists(source_file), source_file\n",
    "    class_name = model_source.callable_name\n",
    "    return f\"{source_file}:{class_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5962d73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first new model: add thresholding of outputs as post-processing\n",
    "# the convenience function `build_model` creates a biomageio model spec compatible package (=zipped folder)\n",
    "from bioimageio.core.build_spec import build_model\n",
    "\n",
    "# create a subfolder to store the files for the new model\n",
    "model_root = \"./new_model\"\n",
    "os.makedirs(model_root, exist_ok=True)\n",
    "\n",
    "# create the expected output tensor (= outputs thresholded at 0.5)\n",
    "threshold = 0.5\n",
    "new_output = prediction > threshold\n",
    "new_output_path = f\"{model_root}/new_test_output.npy\"\n",
    "np.save(new_output_path, new_output)\n",
    "\n",
    "# add thresholding as post-processing procedure to our model\n",
    "preprocessing = [[{\"name\": prep.name, \"kwargs\": prep.kwargs} for prep in inp.preprocessing] for inp in model_resource.inputs]\n",
    "postprocessing = [[{\"name\": \"binarize\", \"kwargs\": {\"threshold\": threshold}}]]\n",
    "\n",
    "# get the model architecture\n",
    "# note that this is only necessary for pytorch state dict models\n",
    "model_source = get_architecture_source(rdf_doi)\n",
    "\n",
    "# we use the `parent` field to indicate that the new model is created based on\n",
    "# the nucleus segmentation model we have obtained from bioimage.io\n",
    "# this field is optional and only needs to be given for models that are created based on other models from bioimage.io\n",
    "# the parent is specified via it's doi and the hash of its rdf file\n",
    "model_root_folder = os.path.split(model_resource.weights[\"pytorch_state_dict\"].source)[0]\n",
    "rdf_file = os.path.join(model_root_folder, \"rdf.yaml\")\n",
    "with open(rdf_file, \"rb\") as f:\n",
    "    rdf_hash = hashlib.sha256(f.read()).hexdigest()\n",
    "parent = {\"uri\": rdf_doi, \"sha256\": rdf_hash}\n",
    "\n",
    "# the name of the new model and where to save the zipped model package\n",
    "name = \"new-model1\"\n",
    "zip_path = os.path.join(model_root, f\"{name}.zip\")\n",
    "\n",
    "# `build_model` needs some additional information about the model, like citation information\n",
    "# all this additional information is passed as plain python types and will be converted into the bioimageio representation internally  \n",
    "# for more informantion, check out the function signature\n",
    "# https://github.com/bioimage-io/core-bioimage-io-python/blob/main/bioimageio/core/build_spec/build_model.py#L252\n",
    "cite = [{\"text\": cite_entry.text, \"url\": cite_entry.url} for cite_entry in model_resource.cite]\n",
    "\n",
    "# the training data used for the model can also be specified by linking to a dataset available on bioimage.io\n",
    "training_data = {\"id\": \"ilastik/stradist_dsb_training_data\"}\n",
    "\n",
    "# the axes descriptions for the inputs / outputs\n",
    "input_axes = [\"bcyx\"]\n",
    "output_axes = [\"bcyx\"]\n",
    "\n",
    "# the pytorch_state_dict weight file\n",
    "weight_file = model_resource.weights[\"pytorch_state_dict\"].source\n",
    "\n",
    "# the path to save the new model with torchscript weights\n",
    "zip_path = f\"{model_root}/new_model2.zip\"\n",
    "\n",
    "# build the model! it will be saved to 'zip_path'\n",
    "new_model_raw = build_model(\n",
    "    weight_uri=weight_file,\n",
    "    test_inputs=model_resource.test_inputs,\n",
    "    test_outputs=[new_output_path],\n",
    "    input_axes=input_axes,\n",
    "    output_axes=output_axes,\n",
    "    output_path=zip_path,\n",
    "    name=name,\n",
    "    description=\"nucleus segmentation model with thresholding\",\n",
    "    authors=[{\"name\": \"Jane Doe\"}],\n",
    "    license=\"CC-BY-4.0\",\n",
    "    documentation=model_resource.documentation,\n",
    "    covers=[str(cover) for cover in model_resource.covers],\n",
    "    tags=[\"nucleus-segmentation\"],\n",
    "    cite=cite,\n",
    "    parent=parent,\n",
    "    architecture=model_source,\n",
    "    model_kwargs=model_resource.weights[\"pytorch_state_dict\"].kwargs,\n",
    "    preprocessing=preprocessing,\n",
    "    postprocessing=postprocessing,\n",
    "    training_data=training_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0616547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the new model from the zipped package, run prediction and check the result\n",
    "new_model = bioimageio.core.load_resource_description(zip_path)\n",
    "prediction = predict_numpy(new_model, input_image)\n",
    "show_images(input_image, prediction, names=[\"input\", \"binarized-prediction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85287bf7",
   "metadata": {},
   "source": [
    "## Add different weight format and package model with new weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482a64bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `convert_weigths_to_pytorch_script` creates torchscript weigths based on the weights loaded from pytorch_state_dict\n",
    "from bioimageio.core.weight_converter.torch import convert_weights_to_torchscript\n",
    "# `add_weights` adds new weights to the model specification\n",
    "from bioimageio.core.build_spec import add_weights\n",
    "\n",
    "# the path to save the newly created torchscript weights\n",
    "weight_path = os.path.join(model_root, \"weights.torchscript\")\n",
    "convert_weights_to_torchscript(new_model, weight_path)\n",
    "\n",
    "# the path to save the new model with torchscript weights\n",
    "zip_path = f\"{model_root}/new_model2.zip\"\n",
    "new_model2_raw = add_weights(new_model_raw, weight_path, weight_type=\"torchscript\", output_path=zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15dd990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the new model from the zipped package, run prediction and check the result\n",
    "new_model = bioimageio.core.load_resource_description(zip_path)\n",
    "prediction = predict_numpy(new_model, input_image, weight_format=\"torchscript\")\n",
    "show_images(input_image, prediction, names=[\"input\", \"binarized-prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa9db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models in the biomageio.core format can also directly be exported as zipped packages\n",
    "# using `bioimageio.core.export_resource_package`\n",
    "bioimageio.core.export_resource_package(new_model2_raw, output_path=\"another_model.zip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
